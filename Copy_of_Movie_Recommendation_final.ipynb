{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ox7hcav2dHt"
      },
      "source": [
        "# üé¨ Simple Hybrid Movie Recommender\n",
        "\n",
        "**Same architecture as the advanced version, but beginner-friendly!**\n",
        "\n",
        "## Architecture Overview:\n",
        "1. **Content-Based Filtering** - Recommends movies with similar descriptions\n",
        "2. **Collaborative Filtering** - Recommends based on what similar users liked\n",
        "3. **Neural Meta-Learner** - Combines both approaches intelligently\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR4DzwIT2dHy"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 1: Install & Import Libraries\n",
        "# ========================================\n",
        "!pip install -q pandas numpy scikit-learn implicit sentence-transformers torch scipy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import implicit\n",
        "from scipy.sparse import csr_matrix\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print('‚úÖ Libraries loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# 02 - Kaggle credentials upload (Colab-specific)\n",
        "# Run this cell and upload kaggle.json when prompted, ONLY if you want the notebook to download the dataset.\n",
        "# If you already uploaded the dataset files into /content/data, skip this cell.\n",
        "from google.colab import files\n",
        "print('If you already uploaded dataset files to /content/data, skip this upload. Otherwise upload kaggle.json now.')\n",
        "uploaded = files.upload() # choose kaggle.json"
      ],
      "metadata": {
        "id": "oBx9rZRD456n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# 02 - Kaggle credentials upload (Colab-specific)\n",
        "# Run this cell and upload kaggle.json when prompted, ONLY if you want the notebook to download the dataset.\n",
        "# If you already uploaded the dataset files into /content/data, skip this cell.\n",
        "from google.colab import files\n",
        "print('If you already uploaded dataset files to /content/data, skip this upload. Otherwise upload kaggle.json now.')\n",
        "uploaded = files.upload() # choose kaggle.json"
      ],
      "metadata": {
        "id": "PgmSS9KW51O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgKycMTP2dH2"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 2: Load Data\n",
        "# ========================================\n",
        "# Upload your CSV files or use sample data\n",
        "\n",
        "# Load movies metadata\n",
        "# Using the (1).csv files as they contain the data uploaded by the user.\n",
        "movies = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
        "movies = movies[['id', 'title', 'overview', 'genres', 'vote_average', 'vote_count']]\n",
        "movies = movies.dropna(subset=['overview', 'title'])\n",
        "movies['id'] = pd.to_numeric(movies['id'], errors='coerce')\n",
        "movies = movies.dropna(subset=['id'])\n",
        "movies['id'] = movies['id'].astype(int)\n",
        "\n",
        "# Load ratings\n",
        "ratings = pd.read_csv('ratings_small.csv')\n",
        "ratings = ratings[['userId', 'movieId', 'rating']]\n",
        "\n",
        "# Keep only movies that have ratings\n",
        "common_movies = set(movies['id']) & set(ratings['movieId'])\n",
        "movies = movies[movies['id'].isin(common_movies)].reset_index(drop=True)\n",
        "ratings = ratings[ratings['movieId'].isin(common_movies)]\n",
        "\n",
        "print(f'‚úÖ Loaded {len(movies)} movies and {len(ratings)} ratings')\n",
        "print(f'   Users: {ratings[\"userId\"].nunique()}')\n",
        "movies.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# 02 - Kaggle credentials upload (Colab-specific)\n",
        "# Run this cell and upload kaggle.json when prompted, ONLY if you want the notebook to download the dataset.\n",
        "# If you already uploaded the dataset files into /content/data, skip this cell.\n",
        "from google.colab import files\n",
        "print('If you already uploaded dataset files to /content/data, skip this upload. Otherwise upload kaggle.json now.')\n",
        "uploaded = files.upload() # choose kaggle.json"
      ],
      "metadata": {
        "id": "Ohcj08kbFEB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load links and map IDs\n",
        "links = pd.read_csv('links_small.csv')\n",
        "\n",
        "# Merge to add movieId (MovieLens ID) to movies dataframe\n",
        "# movies['id'] is TMDB ID, we need to map it to MovieLens movieId\n",
        "merged_movies = movies.merge(\n",
        "    links[['movieId', 'tmdbId']],\n",
        "    left_on='id',\n",
        "    right_on='tmdbId',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Handle potential duplicate column names after merge if movieId already existed.\n",
        "# We want the 'movieId' from the right side of the merge (links data) if suffixes were added,\n",
        "# or the single 'movieId' column if no suffixes were added (first merge).\n",
        "if 'movieId_y' in merged_movies.columns:\n",
        "    # If suffixes were added, take the one from the right (links data) and rename it\n",
        "    merged_movies['movieId'] = merged_movies['movieId_y']\n",
        "    # Drop the suffixed columns that are no longer needed\n",
        "    merged_movies = merged_movies.drop(columns=['movieId_x', 'tmdbId_x', 'movieId_y', 'tmdbId_y'], errors='ignore')\n",
        "elif 'movieId' in merged_movies.columns and 'tmdbId' in merged_movies.columns:\n",
        "    # If no suffixes, it means 'movieId' was newly added directly, and 'tmdbId' is also from the merge.\n",
        "    # This case is when it's the first time the merge truly adds 'movieId' and 'tmdbId'. No renaming needed.\n",
        "    pass # 'movieId' column is already correctly named\n",
        "# If 'movieId' is still missing for some reason, the next dropna will raise an error.\n",
        "\n",
        "movies = merged_movies # Update the global movies DataFrame\n",
        "\n",
        "# Drop rows where we couldn't find a mapping\n",
        "movies = movies.dropna(subset=['movieId'])\n",
        "movies['movieId'] = movies['movieId'].astype(int)\n",
        "\n",
        "print(f'‚úÖ Mapped {len(movies)} movies to MovieLens IDs')"
      ],
      "metadata": {
        "id": "Gc3yndGDLK8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zUvzu92dH4"
      },
      "source": [
        "---\n",
        "## üîµ PART 1: Content-Based Filtering\n",
        "Uses movie descriptions to find similar movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CusCLEVl2dH5"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 3: Create Movie Embeddings\n",
        "# ========================================\n",
        "# Convert movie descriptions into numerical vectors\n",
        "\n",
        "print('Loading sentence transformer model...')\n",
        "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print('Creating embeddings for movie descriptions...')\n",
        "movie_texts = movies['overview'].fillna('').tolist()\n",
        "movie_embeddings = encoder.encode(movie_texts, show_progress_bar=True)\n",
        "\n",
        "print(f'‚úÖ Created embeddings: shape = {movie_embeddings.shape}')\n",
        "print(f'   Each movie is now a vector of {movie_embeddings.shape[1]} numbers')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter movies to only those that exist in ratings\n",
        "ratings_movie_ids = set(ratings['movieId'].unique())\n",
        "movies = movies[movies['movieId'].isin(ratings_movie_ids)]\n",
        "# IMPORTANT: Filter ratings to only include movies present in the now-filtered movies DataFrame\n",
        "ratings = ratings[ratings['movieId'].isin(movies['movieId'])]\n",
        "\n",
        "print(f'‚úÖ Filtered to {len(movies)} movies that have ratings')\n",
        "\n",
        "# IMPORTANT: Rebuild movie_id_to_idx mapping after filtering\n",
        "movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies['id'])}\n",
        "# In the filtering cell, add this after movie_id_to_idx:\n",
        "movies = movies.reset_index(drop=True)  # Reset index after filtering\n",
        "movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies['id'])}\n",
        "movieId_to_idx = {mid: idx for idx, mid in enumerate(movies['movieId'])}\n",
        "\n",
        "\n",
        "# Rebuild movie embeddings for filtered movies\n",
        "print('Rebuilding embeddings for filtered movies...')\n",
        "movie_embeddings = encoder.encode(\n",
        "    movies['overview'].fillna('').tolist(),\n",
        "    show_progress_bar=True\n",
        ")\n",
        "print(f'‚úÖ Embeddings shape: {movie_embeddings.shape}')"
      ],
      "metadata": {
        "id": "-fyTgxLNN6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv371dfY2dH6"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 4: Content-Based Similarity\n",
        "# ========================================\n",
        "\n",
        "def get_content_similar(movie_idx, top_k=10):\n",
        "    \"\"\"\n",
        "    Find movies with similar descriptions.\n",
        "\n",
        "    Args:\n",
        "        movie_idx: Index of the movie in our dataframe\n",
        "        top_k: Number of similar movies to return\n",
        "\n",
        "    Returns:\n",
        "        List of (movie_index, similarity_score) tuples\n",
        "    \"\"\"\n",
        "    # Get the embedding for this movie\n",
        "    movie_vector = movie_embeddings[movie_idx].reshape(1, -1)\n",
        "\n",
        "    # Calculate similarity with all other movies\n",
        "    similarities = cosine_similarity(movie_vector, movie_embeddings)[0]\n",
        "\n",
        "    # Get top similar movies (excluding itself)\n",
        "    similar_indices = similarities.argsort()[::-1][1:top_k+1]\n",
        "\n",
        "    return [(idx, similarities[idx]) for idx in similar_indices]\n",
        "\n",
        "# Test it!\n",
        "test_movie = 'Heat'\n",
        "\n",
        "# Check if the movie exists\n",
        "if test_movie not in movies['title'].values:\n",
        "    print(f'Error: Movie \"{test_movie}\" not found in the dataset. Please choose an existing movie.')\n",
        "else:\n",
        "    test_idx = movies[movies['title'] == test_movie].index[0]\n",
        "    similar = get_content_similar(test_idx, top_k=5)\n",
        "\n",
        "    print(f'\\nüé¨ Movies similar to \"{test_movie}\":')\n",
        "    for idx, score in similar:\n",
        "        print(f'   {movies.iloc[idx][\"title\"]} (similarity: {score:.3f})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZ71Vks2dH7"
      },
      "source": [
        "---\n",
        "## üü¢ PART 2: Collaborative Filtering\n",
        "Uses user ratings to find patterns (\"users who liked X also liked Y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVkO9GJ_2dH7"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 5: Train Collaborative Filter (ALS)\n",
        "# ========================================\n",
        "# Using 'implicit' library instead of Surprise (NumPy 2.0 compatible)\n",
        "\n",
        "print('Training collaborative filtering model (ALS)...')\n",
        "\n",
        "# Create category mappings for users and movies\n",
        "user_cat = ratings['userId'].astype('category')\n",
        "movie_cat = ratings['movieId'].astype('category')\n",
        "\n",
        "# Store mappings for later use\n",
        "user_to_idx = {uid: idx for idx, uid in enumerate(user_cat.cat.categories)}\n",
        "movie_to_idx = {mid: idx for idx, mid in enumerate(movie_cat.cat.categories)}\n",
        "# Add this line after: movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies['id'])}\n",
        "movieId_to_idx = {row['movieId']: idx for idx, (_, row) in enumerate(movies.iterrows())}\n",
        "print(f'‚úÖ Created movieId_to_idx with {len(movieId_to_idx)} entries')\n",
        "\n",
        "idx_to_movie = {idx: mid for mid, idx in movie_to_idx.items()}\n",
        "\n",
        "# Build sparse user-item matrix (users x movies)\n",
        "# implicit expects item-user matrix, so we transpose later\n",
        "user_item_matrix = csr_matrix(\n",
        "    (ratings['rating'], (user_cat.cat.codes, movie_cat.cat.codes))\n",
        ")    #converting ratings into implicit feedback!\n",
        "\n",
        "print(f'   Matrix shape: {user_item_matrix.shape} (users x movies)')\n",
        "\n",
        "# Train ALS model (Alternating Least Squares - similar to SVD)\n",
        "als_model = implicit.als.AlternatingLeastSquares(\n",
        "    factors=100,\n",
        "    iterations=30,\n",
        "    regularization=0.05,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# implicit expects item-user matrix (transpose of user-item)\n",
        "als_model.fit(user_item_matrix.T)\n",
        "\n",
        "print('‚úÖ Collaborative filtering model trained!')\n",
        "print(f'   User factors shape: {als_model.user_factors.shape}')\n",
        "print(f'   Item factors shape: {als_model.item_factors.shape}')\n",
        "\n",
        "def predict_rating(user_id, movie_id):\n",
        "    \"\"\"\n",
        "    Predict how much a user would rate a movie.\n",
        "    \"\"\"\n",
        "    # Get indices\n",
        "    user_idx = user_to_idx.get(user_id)\n",
        "    movie_idx = movie_to_idx.get(movie_id)\n",
        "\n",
        "    if user_idx is None or movie_idx is None:\n",
        "        return 2.5  # Default rating for unknown user/movie\n",
        "\n",
        "    # Add bounds check for the ALS model matrices\n",
        "    if user_idx >= als_model.user_factors.shape[0] or movie_idx >= als_model.item_factors.shape[0]:\n",
        "        return 2.5  # Default rating if index is out of bounds\n",
        "\n",
        "    # Get factors\n",
        "    user_factors = als_model.user_factors[user_idx]\n",
        "    item_factors = als_model.item_factors[movie_idx]\n",
        "\n",
        "    # Convert to numpy arrays if they are implicit.gpu._cuda.Matrix\n",
        "    if hasattr(user_factors, 'to_numpy'):\n",
        "        user_factors = user_factors.to_numpy().flatten()\n",
        "    else:\n",
        "        user_factors = user_factors.flatten()\n",
        "\n",
        "    if hasattr(item_factors, 'to_numpy'):\n",
        "        item_factors = item_factors.to_numpy().flatten()\n",
        "    else:\n",
        "        item_factors = item_factors.flatten()\n",
        "\n",
        "    # Dot product of latent factors gives predicted score\n",
        "    score = np.dot(user_factors, item_factors) #Higher score ‚Üí stronger preference .Used for ranking, not rating accuracy\n",
        "\n",
        "    # Clip to rating scale (0.5 to 5.0)\n",
        "    return np.clip(score, 0.5, 5.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UekavePs2dH8"
      },
      "outputs": [],
      "source": [
        "# Test collaborative filtering\n",
        "test_user = 1\n",
        "test_movie_id = movies.iloc[0]['id']\n",
        "\n",
        "predicted = predict_rating(test_user, test_movie_id)\n",
        "print(f'Predicted rating for User {test_user} on \"{movies.iloc[0][\"title\"]}\": {predicted:.2f}/5.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X81z-jkb2dH9"
      },
      "source": [
        "---\n",
        "## üü£ PART 3: Neural Meta-Learner\n",
        "A small neural network that learns to combine content + collaborative signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpIP_aDK2dH9"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 6: Prepare Training Data for Meta-Learner\n",
        "# ========================================\n",
        "\n",
        "# Create mappings for quick lookup\n",
        "tmdb_id_to_df_idx = {mid: idx for idx, mid in enumerate(movies['id'])}\n",
        "movielens_id_to_df_idx = {mlid: idx for idx, mlid in enumerate(movies['movieId'])}\n",
        "\n",
        "# Pre-compute user profiles (average of movies they liked)\n",
        "print('Building user profiles...')\n",
        "user_profiles = {}\n",
        "for user_id in tqdm(ratings['userId'].unique()):\n",
        "    user_ratings = ratings[ratings['userId'] == user_id]\n",
        "    liked = user_ratings[user_ratings['rating'] >= 4.0]['movieId'].tolist()\n",
        "    if liked:\n",
        "        # Correctly map MovieLens IDs from 'liked' list to DataFrame indices for movie_embeddings\n",
        "        indices = [movieId_to_idx[m] for m in liked if m in movieId_to_idx]\n",
        "        if indices:\n",
        "            user_profiles[user_id] = movie_embeddings[indices].mean(axis=0)\n",
        "\n",
        "# Parse genres from movies\n",
        "import ast\n",
        "def parse_genres(genres_str):\n",
        "    try:\n",
        "        genres = ast.literal_eval(genres_str)\n",
        "        return [g['name'] for g in genres]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "movies['genre_list'] = movies['genres'].apply(parse_genres)\n",
        "all_genres = set(g for genres in movies['genre_list'] for g in genres)\n",
        "print(f'Found {len(all_genres)} unique genres')\n",
        "\n",
        "# Build user genre preferences\n",
        "user_genre_prefs = {}\n",
        "for user_id in ratings['userId'].unique():\n",
        "    user_ratings = ratings[ratings['userId'] == user_id]\n",
        "    liked_movies = user_ratings[user_ratings['rating'] >= 4.0]['movieId'].tolist()\n",
        "    genre_counts = {}\n",
        "    for mid in liked_movies:\n",
        "        movie_row = movies[movies['id'] == mid] # mid here is MovieLens ID, but movies['id'] is TMDB ID\n",
        "        # Need to find movie row by MovieLens ID\n",
        "        movie_row = movies[movies['movieId'] == mid]\n",
        "        if len(movie_row) > 0:\n",
        "            for g in movie_row.iloc[0]['genre_list']:\n",
        "                genre_counts[g] = genre_counts.get(g, 0) + 1\n",
        "    user_genre_prefs[user_id] = genre_counts\n",
        "\n",
        "def compute_features(user_id, movie_id):\n",
        "    \"\"\"\n",
        "    Compute 8 features for the meta-learner.\n",
        "\n",
        "    Features:\n",
        "    1. CF score - Collaborative filtering prediction\n",
        "    2. Popularity - How many people rated this movie (log scaled)\n",
        "    3. Avg rating - Average rating of the movie\n",
        "    4. Content similarity - How similar to user's liked movies\n",
        "    5. Genre overlap - How many genres match user preferences\n",
        "    6. User activity - How many movies user has rated\n",
        "    7. Rating variance - How consistent is this movie's ratings\n",
        "    8. Recency bias - Newer movies get slight boost\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # Feature 1: Collaborative filtering score\n",
        "    # Need to pass MovieLens ID to predict_rating\n",
        "    movie_row_df = movies[movies['id'] == movie_id] # movie_id passed here is TMDB ID\n",
        "    if len(movie_row_df) == 0:\n",
        "        # If TMDB ID not found, return default features\n",
        "        return [0.5] + [0.5]*7 # Return 8 features\n",
        "    current_movie_movielens_id = movie_row_df.iloc[0]['movieId']\n",
        "    cf_score = predict_rating(user_id, current_movie_movielens_id) / 5.0\n",
        "    features.append(cf_score)\n",
        "\n",
        "    # Get movie data\n",
        "    movie_row = movies[movies['id'] == movie_id]\n",
        "    movie_idx = tmdb_id_to_df_idx.get(movie_id)\n",
        "\n",
        "    if len(movie_row) > 0 and movie_idx is not None:\n",
        "        row = movie_row.iloc[0]\n",
        "\n",
        "        # Feature 2: Popularity (log scaled)\n",
        "        popularity = np.log1p(row['vote_count']) / 15\n",
        "        features.append(min(popularity, 1.0))\n",
        "\n",
        "        # Feature 3: Average rating\n",
        "        avg_rating = row['vote_average'] / 10\n",
        "        features.append(avg_rating)\n",
        "\n",
        "        # Feature 4: Content similarity to user profile\n",
        "        if user_id in user_profiles:\n",
        "            content_sim = cosine_similarity(\n",
        "                movie_embeddings[movie_idx].reshape(1, -1),\n",
        "                user_profiles[user_id].reshape(1, -1)\n",
        "            )[0][0]\n",
        "        else:\n",
        "            content_sim = 0.5\n",
        "        features.append(content_sim)\n",
        "\n",
        "        # Feature 5: Genre overlap with user preferences\n",
        "        movie_genres = set(row['genre_list'])\n",
        "        user_prefs = user_genre_prefs.get(user_id, {})\n",
        "        if user_prefs and movie_genres:\n",
        "            overlap = sum(user_prefs.get(g, 0) for g in movie_genres)\n",
        "            genre_score = min(overlap / 10, 1.0)\n",
        "        else:\n",
        "            genre_score = 0.5\n",
        "        features.append(genre_score)\n",
        "\n",
        "        # Feature 6: User activity level\n",
        "        user_count = len(ratings[ratings['userId'] == user_id])\n",
        "        user_activity = min(np.log1p(user_count) / 5, 1.0)\n",
        "        features.append(user_activity)\n",
        "\n",
        "        # Feature 7: Rating consistency (inverse of variance)\n",
        "        movie_ratings = ratings[ratings['movieId'] == current_movie_movielens_id]['rating'] # Use MovieLens ID here\n",
        "        if len(movie_ratings) > 1:\n",
        "            variance = movie_ratings.var()\n",
        "            consistency = 1 / (1 + variance)\n",
        "        else:\n",
        "            consistency = 0.5\n",
        "        features.append(consistency)\n",
        "\n",
        "        # Feature 8: Vote count ratio (quality signal)\n",
        "        vote_ratio = row['vote_count'] / (row['vote_count'] + 100)  # Bayesian smoothing\n",
        "        features.append(vote_ratio)\n",
        "    else:\n",
        "        # Default values if movie not found (excluding the CF score which is already handled)\n",
        "        features.extend([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) # Already has cf_score\n",
        "\n",
        "    return features\n",
        "\n",
        "FEATURE_NAMES = ['CF Score', 'Popularity', 'Avg Rating', 'Content Sim',\n",
        "                 'Genre Overlap', 'User Activity', 'Rating Consistency', 'Vote Ratio']\n",
        "print(f'\\n‚úÖ Using {len(FEATURE_NAMES)} features: {FEATURE_NAMES}')\n",
        "\n",
        "print('Creating training data for meta-learner...')\n",
        "\n",
        "# Sample some user-movie pairs for training\n",
        "sample_ratings = ratings.sample(n=min(50000, len(ratings)), random_state=SEED)\n",
        "\n",
        "X = []  # Features\n",
        "y = []  # Labels (actual ratings)\n",
        "\n",
        "for _, row in tqdm(sample_ratings.iterrows(), total=len(sample_ratings)):\n",
        "    # Pass TMDB ID to compute_features, which expects it\n",
        "    tmdb_movie_id = movies[movies['movieId'] == row['movieId']]['id'].iloc[0] if row['movieId'] in movies['movieId'].values else None\n",
        "    if tmdb_movie_id is not None:\n",
        "        features = compute_features(row['userId'], tmdb_movie_id)\n",
        "        X.append(features)\n",
        "        y.append(row['rating'] / 5.0)  # Normalize rating to 0-1\n",
        "\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.float32).reshape(-1, 1)\n",
        "\n",
        "print(f'‚úÖ Created {len(X)} training samples with {X.shape[1]} features each')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmEwdvgW2dH-"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 7: Define the Neural Network\n",
        "# ========================================\n",
        "\n",
        "# Replace the entire class with:\n",
        "class MetaLearner(nn.Module):\n",
        "    def __init__(self, n_features=8):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(n_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model = MetaLearner(n_features=X.shape[1])\n",
        "print(f'‚úÖ Model created with {X.shape[1]} input features!')\n",
        "print(f'   Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_hIBSOk2dH_"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 8: Train the Meta-Learner\n",
        "# ========================================\n",
        "\n",
        "# Split data: 80% train, 10% validation, 10% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.11, random_state=SEED)\n",
        "\n",
        "print(f'Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}')\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_t = torch.from_numpy(X_train)\n",
        "y_train_t = torch.from_numpy(y_train)\n",
        "X_val_t = torch.from_numpy(X_val)\n",
        "y_val_t = torch.from_numpy(y_val)\n",
        "X_test_t = torch.from_numpy(X_test)\n",
        "y_test_t = torch.from_numpy(y_test)\n",
        "\n",
        "# Training setup\n",
        "loss_fn = nn.MSELoss()  # Mean Squared Error\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 50\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print('\\nTraining...')\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train_t)\n",
        "    train_loss = loss_fn(predictions, y_train_t)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_predictions = model(X_val_t)\n",
        "        val_loss = loss_fn(val_predictions, y_val_t)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
        "\n",
        "print('\\n‚úÖ Training complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR1CKjGy2dH_"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 9: Evaluate Model Fit\n",
        "# ========================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate test loss\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_predictions = model(X_test_t)\n",
        "    test_loss = loss_fn(test_predictions, y_test_t).item()\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "plt.plot(val_losses, label='Val Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "bars = plt.bar(['Train', 'Val', 'Test'],\n",
        "               [train_losses[-1], val_losses[-1], test_loss],\n",
        "               color=['blue', 'orange', 'green'])\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Final Loss Comparison')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Diagnosis\n",
        "print('\\nüìä MODEL FIT DIAGNOSIS:')\n",
        "print(f'   Train Loss: {train_losses[-1]:.4f}')\n",
        "print(f'   Val Loss:   {val_losses[-1]:.4f}')\n",
        "print(f'   Test Loss:  {test_loss:.4f}')\n",
        "\n",
        "gap = val_losses[-1] - train_losses[-1]\n",
        "if gap > 0.05:\n",
        "    print('\\n‚ö†Ô∏è Possible OVERFITTING - model memorized training data')\n",
        "    print('   Try: more dropout, less epochs, or more data')\n",
        "elif train_losses[-1] > 0.1:\n",
        "    print('\\n‚ö†Ô∏è Possible UNDERFITTING - model is too simple')\n",
        "    print('   Try: more layers, more neurons, or more epochs')\n",
        "else:\n",
        "    print('\\n‚úÖ Model looks WELL-GENERALIZED!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu9nm2aU2dIA"
      },
      "source": [
        "---\n",
        "## üéØ PART 4: Make Recommendations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjAG5NZb2dIA"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 10: Recommendation Function\n",
        "# ========================================\n",
        "\"\"\"\n",
        "    Get movie recommendations for a user.\n",
        "\n",
        "    Process:\n",
        "    1. Get movies the user hasn't rated\n",
        "    2. Compute features for each candidate\n",
        "    3. Score with meta-learner\n",
        "    4. Return top N\n",
        "    \"\"\"\n",
        "\n",
        "def recommend_for_user(user_id, n_recommendations=10, exclude_movie_ids=None):\n",
        "    \"\"\"\n",
        "    Get movie recommendations for a user.\n",
        "\n",
        "    Args:\n",
        "        user_id: The ID of the user to recommend for.\n",
        "        n_recommendations: Number of recommendations to return.\n",
        "        exclude_movie_ids: A set of MovieLens IDs to explicitly exclude from recommendations (e.g., movies already rated by the user in the training set).\n",
        "    \"\"\"\n",
        "    if exclude_movie_ids is None:\n",
        "        # Default exclusion: movies user already rated (global ratings)\n",
        "        user_rated = set(ratings[ratings['userId'] == user_id]['movieId'].tolist())\n",
        "    else:\n",
        "        # Use provided exclusion set during evaluation\n",
        "        user_rated = exclude_movie_ids\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for idx, row in movies.iterrows():\n",
        "        movie_id = row['id']  # TMDB ID for feature computation\n",
        "        movielens_id = row['movieId']  # MovieLens ID for evaluation\n",
        "\n",
        "        if movielens_id in user_rated:\n",
        "            continue\n",
        "\n",
        "        # Compute features and score\n",
        "        features = compute_features(user_id, movie_id)\n",
        "        features_tensor = torch.tensor([features], dtype=torch.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval() # Ensure model is in evaluation mode\n",
        "            score = model(features_tensor).item()\n",
        "\n",
        "        # Return MovieLens ID (not TMDB ID) so it matches ground truth\n",
        "        scores.append((row['title'], score, movielens_id))\n",
        "\n",
        "    # Sort by score and return top N\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scores[:n_recommendations]\n",
        "\n",
        "# Test recommendations\n",
        "test_user = 100\n",
        "\n",
        "recommendations = recommend_for_user(test_user, n_recommendations=10)\n",
        "\n",
        "print(f'\\nüé¨ Top 10 Recommendations for User {test_user}:')\n",
        "print('-' * 50)\n",
        "\n",
        "# For testing, we still exclude all previously rated movies for a clean recommendation list\n",
        "recommendations = recommend_for_user(test_user, n_recommendations=10)\n",
        "for i, (title, score, _) in enumerate(recommendations, 1):\n",
        "    print(f'{i:2}. {title[:40]:<40} (score: {score:.3f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KSNmSAL2dIB"
      },
      "source": [
        "---\n",
        "## üìà Visualize Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSCz8q0b2dIA"
      },
      "outputs": [],
      "source": [
        "def evaluate_properly(n_users=100, k_values=[5, 10, 20]):\n",
        "    np.random.seed(42)  # Add this line at the start\n",
        "\n",
        "\n",
        "    \"\"\"Proper evaluation - temporarily hide test movies from user history.\"\"\"\n",
        "    global eval_results\n",
        "\n",
        "    # Initialize eval_results with all metrics including accuracy\n",
        "    eval_results = {k: {'precision': [], 'recall': [], 'f1': [], 'hit': [], 'ndcg': [], 'accuracy': []} for k in k_values}\n",
        "    movies_set = set(movies['movieId'].values)\n",
        "\n",
        "    # Get users with enough high ratings in our movie set\n",
        "    user_high_ratings = ratings[(ratings['rating'] >= 4.0) &\n",
        "                                 (ratings['movieId'].isin(movies_set))].groupby('userId')['movieId'].apply(list).to_dict()\n",
        "\n",
        "    valid_users = [u for u, m in user_high_ratings.items() if len(m) >= 5]\n",
        "    sample_users = np.random.choice(valid_users, size=min(n_users, len(valid_users)), replace=False)\n",
        "\n",
        "    print(f'Evaluating {len(sample_users)} users with 5+ high-rated movies in our set')\n",
        "\n",
        "    for user_id in tqdm(sample_users, desc='Evaluating'):\n",
        "        user_high = user_high_ratings[user_id]\n",
        "\n",
        "        # Hold out 20% as test\n",
        "        np.random.shuffle(user_high)\n",
        "        n_test = max(1, len(user_high) // 5)\n",
        "        test_movies = set(user_high[:n_test])\n",
        "\n",
        "        # Get movies user rated (excluding test set)\n",
        "        all_rated = set(ratings[ratings['userId'] == user_id]['movieId'].tolist())\n",
        "        train_rated = all_rated - test_movies\n",
        "\n",
        "        # Score all candidate movies\n",
        "        scores = []\n",
        "        for idx, row in movies.iterrows():\n",
        "            mid = row['movieId']\n",
        "            if mid in train_rated:\n",
        "                continue\n",
        "            features = compute_features(user_id, row['id'])\n",
        "            features_tensor = torch.tensor([features], dtype=torch.float32)\n",
        "            with torch.no_grad():\n",
        "                score = model(features_tensor).item()\n",
        "            scores.append((mid, score))\n",
        "\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        rec_ids = [s[0] for s in scores[:max(k_values)]]\n",
        "\n",
        "        for k in k_values:\n",
        "            top_k = set(rec_ids[:k])\n",
        "            hits = len(top_k & test_movies)\n",
        "\n",
        "            # Precision\n",
        "            precision = hits / k if k > 0 else 0\n",
        "            eval_results[k]['precision'].append(precision)\n",
        "\n",
        "            # Recall\n",
        "            recall = hits / len(test_movies) if len(test_movies) > 0 else 0\n",
        "            eval_results[k]['recall'].append(recall)\n",
        "\n",
        "            # F1 Score\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0\n",
        "            eval_results[k]['f1'].append(f1)\n",
        "\n",
        "            # Hit Rate\n",
        "            eval_results[k]['hit'].append(1 if hits > 0 else 0)\n",
        "\n",
        "            # Accuracy\n",
        "            accuracy = hits / min(k, len(test_movies)) if min(k, len(test_movies)) > 0 else 0\n",
        "            eval_results[k]['accuracy'].append(accuracy)\n",
        "\n",
        "            # NDCG\n",
        "            ndcg = 0\n",
        "            if hits > 0 and len(test_movies) > 0:\n",
        "                for rank, rec_id in enumerate(rec_ids[:k]):\n",
        "                    if rec_id in test_movies:\n",
        "                        ndcg += 1 / np.log2(rank + 2)\n",
        "                ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(test_movies), k)))\n",
        "                if ideal_dcg > 0:\n",
        "                    ndcg /= ideal_dcg\n",
        "            eval_results[k]['ndcg'].append(ndcg)\n",
        "\n",
        "    print('='*60)\n",
        "    print('üìä RESULTS')\n",
        "    print('='*60)\n",
        "    for k in k_values:\n",
        "        print(f\"@{k}: Precision={np.mean(eval_results[k]['precision']):.4f}, \"\n",
        "              f\"Recall={np.mean(eval_results[k]['recall']):.4f}, \"\n",
        "              f\"F1={np.mean(eval_results[k]['f1']):.4f}, \"\n",
        "              f\"Hit Rate={np.mean(eval_results[k]['hit']):.4f}, \"\n",
        "              f\"NDCG={np.mean(eval_results[k]['ndcg']):.4f}, \"\n",
        "              f\"Accuracy={np.mean(eval_results[k]['accuracy']):.4f}\")\n",
        "\n",
        "evaluate_properly(n_users=50)\n",
        "\n",
        "# ========================================\n",
        "# Plotting\n",
        "# ========================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k_values = [5, 10, 20]\n",
        "metrics = ['precision', 'recall', 'f1', 'hit', 'ndcg', 'accuracy']\n",
        "metric_labels = ['Precision', 'Recall', 'F1 Score', 'Hit Rate', 'NDCG', 'Accuracy']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Metrics by K\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(k_values))\n",
        "width = 0.12\n",
        "for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
        "    values = [np.mean(eval_results[k][metric]) for k in k_values]\n",
        "    ax1.bar(x + i*width, values, width, label=label)\n",
        "\n",
        "ax1.set_xlabel('K (Top-K Recommendations)')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Recommendation Metrics at Different K Values')\n",
        "ax1.set_xticks(x + width * 2.5)\n",
        "ax1.set_xticklabels([f'@{k}' for k in k_values])\n",
        "ax1.legend(loc='upper right')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Precision-Recall Trade-off\n",
        "ax2 = axes[1]\n",
        "precisions = [np.mean(eval_results[k]['precision']) for k in k_values]\n",
        "recalls = [np.mean(eval_results[k]['recall']) for k in k_values]\n",
        "\n",
        "ax2.plot(recalls, precisions, 'bo-', markersize=10, linewidth=2)\n",
        "for i, k in enumerate(k_values):\n",
        "    ax2.annotate(f'@{k}', (recalls[i], precisions[i]),\n",
        "                 textcoords='offset points', xytext=(10, 5), fontsize=12)\n",
        "\n",
        "ax2.set_xlabel('Recall')\n",
        "ax2.set_ylabel('Precision')\n",
        "ax2.set_title('Precision-Recall Trade-off')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print('\\n' + '='*60)\n",
        "print('üìä AVERAGE EVALUATION RESULTS')\n",
        "print('='*60)\n",
        "for k in k_values:\n",
        "    print(f\"@{k}: Precision={np.mean(eval_results[k]['precision']):.4f}, \"\n",
        "          f\"Recall={np.mean(eval_results[k]['recall']):.4f}, \"\n",
        "          f\"F1={np.mean(eval_results[k]['f1']):.4f}, \"\n",
        "          f\"Hit Rate={np.mean(eval_results[k]['hit']):.4f}, \"\n",
        "          f\"NDCG={np.mean(eval_results[k]['ndcg']):.4f}, \"\n",
        "          f\"Accuracy={np.mean(eval_results[k]['accuracy']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug check\n",
        "print(f\"Movies count: {len(movies)}\")\n",
        "print(f\"movie_id_to_idx size: {len(movie_id_to_idx)}\")\n",
        "print(f\"movieId_to_idx size: {len(movieId_to_idx)}\")\n",
        "print(f\"movie_embeddings shape: {movie_embeddings.shape}\")\n",
        "print(f\"User profiles count: {len(user_profiles)}\")\n",
        "\n",
        "# Check if user profiles are being built correctly\n",
        "test_user = list(user_profiles.keys())[0] if user_profiles else None\n",
        "print(f\"Sample user profile exists: {test_user is not None}\")\n"
      ],
      "metadata": {
        "id": "uuNW5W3Xse_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# SAVE MODEL AND DATA FOR STREAMLIT\n",
        "# ========================================\n",
        "import pickle\n",
        "import torch # Add this import statement\n",
        "\n",
        "# Save the trained PyTorch model\n",
        "torch.save(model.state_dict(), 'meta_learner_weights.pt')\n",
        "\n",
        "# Save embeddings\n",
        "np.save('movie_embeddings.npy', movie_embeddings)\n",
        "\n",
        "# Save dataframes\n",
        "movies.to_csv('movies_filtered.csv', index=False)\n",
        "ratings.to_csv('ratings_filtered.csv', index=False)\n",
        "\n",
        "# Save all mappings and user data\n",
        "mappings = {\n",
        "    'user_to_idx': user_to_idx,\n",
        "    'movie_to_idx': movie_to_idx,\n",
        "    'movie_id_to_idx': movie_id_to_idx,\n",
        "    'movieId_to_idx': movieId_to_idx,\n",
        "    'user_profiles': user_profiles,\n",
        "    'user_genre_prefs': user_genre_prefs,\n",
        "    'all_genres': all_genres\n",
        "}\n",
        "\n",
        "with open('mappings.pkl', 'wb') as f:\n",
        "    pickle.dump(mappings, f)\n",
        "\n",
        "# Save ALS model factors\n",
        "# Convert to numpy arrays before saving\n",
        "np.save('als_user_factors.npy', als_model.user_factors.to_numpy())\n",
        "np.save('als_item_factors.npy', als_model.item_factors.to_numpy())\n",
        "\n",
        "print('‚úÖ All files saved!')\n",
        "print('Files created:')\n",
        "print('  - meta_learner_weights.pt')\n",
        "print('  - movie_embeddings.npy')\n",
        "print('  - movies_filtered.csv')\n",
        "print('  - ratings_filtered.csv')\n",
        "print('  - mappings.pkl')\n",
        "print('  - als_user_factors.npy')\n",
        "print('  - als_item_factors.npy')"
      ],
      "metadata": {
        "id": "TswrhXuwjSTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9115PGnH2dIC"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}